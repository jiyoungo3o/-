!pip install konlpy   # 셀에서 먼저 실행하세요!


# 2. 라이브러리 임포트
import pandas as pd
import re

# 1. 데이터 불러오기 (경로는 환경에 맞게 수정)
df = pd.read_csv('파일 이름')

# 5. 컬럼명 확인 및 조정 ('content' → 'text' 변경 필요 시)
print("원본 컬럼명:", df.columns)
print("데이터 샘플:\n", df.head())

if 'content' in df.columns and 'text' not in df.columns:
    df = df.rename(columns={'content': 'text'})

# 6. 사용할 컬럼만 선택 ('publishedAt'이 있으면 포함)
columns_to_keep = ['video_id', 'author', 'text']
if 'publishedAt' in df.columns:
    columns_to_keep.append('publishedAt')

available_cols = [col for col in columns_to_keep if col in df.columns]
df = df[available_cols]

# 7. 중복 댓글 제거 (video_id + text 기준)
df = df.drop_duplicates(subset=['video_id', 'text'])

# 8. 결측값 처리 (text가 없으면 제거, 빈값은 빈 문자열로)
df = df.dropna(subset=['text'])
df['text'] = df['text'].fillna('')

# 9. 텍스트 전체 소문자 변환
df['text'] = df['text'].str.lower()

# 10. 욕설 단어 리스트 (필요에 맞게 추가/수정하세요)
curse_words = [
    '',''
    # 실제 사용하실 욕설 단어로 바꾸세요
]

# 욕설 제거 함수: 욕설 단어를 '*'로 대체
def remove_curse(text):
    pattern = '|'.join(map(re.escape, curse_words))
    return re.sub(pattern, '*', text, flags=re.IGNORECASE)

# 11. 텍스트 클리닝 함수: 시간좌표, 숫자단어, URL, 멘션, 웃음표현, 욕설, 특수문자 제거
def clean_text(text):
    # 시:분:초 형식 제거 (예: 1:02:30)
    text = re.sub(r'\b\d{1,2}:\d{1,2}:\d{1,2}\b', '', text)

    # 분:초 형식 제거 (예: 0:15)
    text = re.sub(r'\b\d{1,2}:\d{1,2}\b', '', text)

    # 단독 숫자 단어 제거 (예: 358)
    text = re.sub(r'\b\d+\b', '', text)

    # URL 제거 (http, https, www 포함)
    text = re.sub(r'https?://\S+|www\.\S+', '', text)

    # 유저 멘션 (@username 형태) 제거 (공백 전까지 모두 제거)
    text = re.sub(r'@\S+', '', text)

    # 한글 웃음표현 제거 (ㅋㅋㅋ, ㅎㅎㅎ 등)
    text = re.sub(r'([ㅋㅎ]{2,})', '', text)

    # 영어 웃음표현 제거 (zzzz, lol, haha 등)
    text = re.sub(r'(.)\1{2,}', '', text)

    # 욕설 단어 제거 (별표로 대체)
    text = remove_curse(text)

    # 한글, 영어, 공백만 남기고 특수문자 및 이모지 제거
    text = re.sub(r'[^가-힣a-zA-Z\s]', '', text)

    # 다중 공백 → 단일 공백, 앞뒤 공백 제거
    text = re.sub(r'\s+', ' ', text).strip()

    return text

# 복합어 띄어쓰기 치환 함수 정의
def replace_phrase(text):
    return text.replace('칸나로 마트', '칸나로마트')

# 클리닝 전에 먼저 적용
df['text'] = df['text'].apply(replace_phrase)

# 기존 클리닝 함수 적용
df['text'] = df['text'].apply(clean_text)

# 12. 클리닝 적용
df['text'] = df['text'].apply(clean_text)

# 13. 너무 짧은 댓글(길이 2자 이하) 제거
df = df[df['text'].str.len() > 2]

# 14. 인덱스 초기화
df = df.reset_index(drop=True)

# 15. 전처리 결과 저장 (코랩 작업 디렉토리에)
df.to_csv('youtube_comments_preprocessed.csv', index=False, encoding='utf-8-sig')

# 16. 토씨(조사·어미 등) 제거 - 형태소 분석 후 불필요 품사 제거
from konlpy.tag import Okt
okt = Okt()

# 조사를 포함한 불필요 품사 태그 리스트
stop_pos = ['J', 'E', 'X']  # J: 조사, E: 어미, X: 접두/접미/보조용언

def remove_stopwords(text):
    # 형태소 단위[(단어, 품사)] 분석
    morphs = okt.pos(text, stem=True)  # 어간 추출
    # 불용어 품사와 불용어 리스트에 없는 것만 남김
    return ' '.join([word for word, pos in morphs
                     if (pos[0] not in stop_pos) and (word not in stopwords)])

# 적용
df['text_nostop'] = df['text'].apply(remove_stopwords)

# 결과 예시 출력
print(df[['text', 'text_nostop']].head())


print("전처리 완료! 'youtube_comments_preprocessed.csv' 파일로 저장되었습니다.")
print(df.head())
